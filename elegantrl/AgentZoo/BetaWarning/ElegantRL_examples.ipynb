{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The totorial version and formal version\n",
    "You can find a simple version in `elegantrl/tutorial/run.py`\n",
    "You can also find demo 1~3 in `elegantrl/run.py` (advanced version)\n",
    "\n",
    "elegantrl/tutorial <1000 lines \n",
    "```\n",
    "agent.py # 530 lines\n",
    "net.py   # 160 lines\n",
    "run.py   # 320 lines\n",
    "env.py   # 160 lines (not necessary)\n",
    "```\n",
    "The structtion of formal version is similar to tutorial version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elegantrl.tutorial.run import Arguments, train_and_evaluate\n",
    "from elegantrl.tutorial.env import PreprocessEnv\n",
    "import gym\n",
    "gym.logger.set_level(40)  # Block warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Discrete action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''choose an DRL algorithm'''\n",
    "from elegantrl.tutorial.agent import AgentDoubleDQN  # AgentDQN\n",
    "\n",
    "args = Arguments(agent=None, env=None, gpu_id=None)\n",
    "args.agent = AgentDoubleDQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| env_name: CartPole-v0, action space if_discrete: True\n",
      "| state_dim: 4, action_dim: 2, action_max: 1\n",
      "| max_step: 200 target_reward: 195.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TotalStep: 2e3, TargetReward: , UsedTime: 10s'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''choose environment'''\n",
    "args.env = PreprocessEnv(env=gym.make('CartPole-v0'))\n",
    "args.net_dim = 2 ** 7  # change a default hyper-parameters\n",
    "args.batch_size = 2 ** 7\n",
    "\"TotalStep: 2e3, TargetReward: , UsedTime: 10s\"\n",
    "\n",
    "# args.env = PreprocessEnv(env=gym.make('LunarLander-v2'))\n",
    "# args.net_dim = 2 ** 8\n",
    "# args.batch_size = 2 ** 8\n",
    "# \"TotalStep: 6e4, TargetReward: 200, UsedTime: 600s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| GPU id: 0, cwd: ./CartPole-v0_0\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "0   0.00e+00    200.00 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "0   1.02e+03    195.00 |  200.00      0.00         12  ########\n"
     ]
    }
   ],
   "source": [
    "'''train and evaluate'''\n",
    "train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Continuous action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DEMO 2.1: choose an off-policy DRL algorithm'''\n",
    "from elegantrl.agent import AgentSAC  # AgentTD3, AgentDDPG\n",
    "args = Arguments(if_on_policy=False)\n",
    "args.agent = AgentSAC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DEMO 2.2: choose an on-policy DRL algorithm'''\n",
    "from elegantrl.tutorial.agent import AgentPPO \n",
    "args = Arguments(if_on_policy=True)  # hyper-parameters of on-policy is different from off-policy\n",
    "args.agent = AgentPPO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| env_name: Pendulum-v0, action space if_discrete: False\n",
      "| state_dim: 3, action_dim: 1, action_max: 2.0\n",
      "| max_step: 200 target_reward: -200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TotalStep: 2e5, TargetReward: 300, UsedTime: 5000s'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''choose environment'''\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.target_reward = -200  # set target_reward manually for env 'Pendulum-v0'\n",
    "args.env = PreprocessEnv(env=env)\n",
    "args.reward_scale = 2 ** -3  # RewardRange: -1800 < -200 < -50 < 0\n",
    "args.net_dim = 2 ** 7\n",
    "args.batch_size = 2 ** 7\n",
    "\"TotalStep: 3e5, TargetReward: -200, UsedTime: 300s\"\n",
    "\n",
    "# args.env = PreprocessEnv(env=gym.make('LunarLanderContinuous-v2'))\n",
    "# args.reward_scale = 2 ** 0  # RewardRange: -800 < -200 < 200 < 302\n",
    "# \"TotalStep: 9e4, TargetReward: 200, UsedTime: 2500s\"\n",
    "\n",
    "# args.env = PreprocessEnv(env=gym.make('BipedalWalker-v3'))\n",
    "# args.reward_scale = 2 ** 0  # RewardRange: -200 < -150 < 300 < 334\n",
    "# args.break_step = int(2e5)\n",
    "# args.if_allow_break = False\n",
    "# \"TotalStep: 2e5, TargetReward: 300, UsedTime: 5000s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: Custom Env from AI4Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TotalStep: 10e5, TargetReward: 1.62, UsedTime: 1000s'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Arguments(if_on_policy=True)\n",
    "'''choose an DRL algorithm'''\n",
    "from elegantrl.tutorial.agent import AgentPPO\n",
    "args.agent = AgentPPO()\n",
    "\n",
    "from elegantrl.tutorial.env import FinanceMultiStockEnv  # a standard env for ElegantRL, not need PreprocessEnv()\n",
    "args.env = FinanceMultiStockEnv(if_train=True)\n",
    "args.env_eval = FinanceMultiStockEnv(if_train=False)  # eva_len = 1699 - train_len\n",
    "args.reward_scale = 2 ** 0  # RewardRange: 0 < 1.0 < 1.25 <\n",
    "args.break_step = int(5e6)\n",
    "args.max_step = args.env.max_step\n",
    "args.max_memo = (args.max_step - 1) * 8\n",
    "args.batch_size = 2 ** 11\n",
    "\"TotalStep:  2e5, TargetReward: 1.25, UsedTime:  200s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| GPU id: 0, cwd: ./FinanceStock-v2_0\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "0   0.00e+00      1.06 |\n",
      "0   5.12e+03      1.10 |\n",
      "0   1.02e+04      1.22 |\n",
      "0   2.56e+04      1.29 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "0   3.07e+04      1.25 |    1.29      0.03         29  ########\n"
     ]
    }
   ],
   "source": [
    "'''train and evaluate'''\n",
    "train_and_evaluate(args)\n",
    "# args.rollout_num = 8\n",
    "# train_and_evaluate__multiprocessing(args)  # try multiprocessing in formal version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 4: train in PyBullet (MuJoCo) (wait for adding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elegantrl.run import Arguments, train_and_evaluate__multiprocessing\n",
    "from elegantrl.env import PreprocessEnv\n",
    "\n",
    "import gym  # don't worry about 'WARN: Box bound precision lowered by casting to float32'\n",
    "import pybullet_envs  # Free PyBullet as an env alternatives of paid MuJoCo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| env_name:  AntBulletEnv-v0, action space if_discrete: False\n",
      "| state_dim:   28, action_dim: 8, action_max: 1.0\n",
      "| max_step:  1000, target_reward: 2500.0\n"
     ]
    }
   ],
   "source": [
    "'''DEMO 4.1: choose an off-policy DRL algorithm'''\n",
    "args = Arguments(if_on_policy=False)\n",
    "\n",
    "from elegantrl.agent import AgentModSAC  # AgentSAC, AgentTD3, AgentDDPG\n",
    "args.agent = AgentModSAC()  # AgentSAC(), AgentTD3(), AgentDDPG()\n",
    "args.agent.if_use_dn = True\n",
    "args.net_dim = 2 ** 7  # default is 2 ** 8 is too large for if_use_dn = True\n",
    "\n",
    "env_name = 'AntBulletEnv-v0'\n",
    "assert env_name in {\"AntBulletEnv-v0\",\n",
    "                    \"Walker2DBulletEnv-v0\", \n",
    "                    \"HalfCheetahBulletEnv-v0\",\n",
    "                    \"HumanoidBulletEnv-v0\", \n",
    "                    \"HumanoidFlagrunBulletEnv-v0\", \n",
    "                    \"HumanoidFlagrunHarderBulletEnv-v0\",}\n",
    "args.env = PreprocessEnv(env=gym.make(env_name))\n",
    "args.env.max_step = 2 ** 10\n",
    "args.env.target_reward = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| multiprocessing, act_workers: 4\n",
      "| multiprocessing, None:\n",
      "| GPU id: 0, cwd: ./AgentModSAC/AntBulletEnv-v0_0\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "0   0.00e+00    453.28 |\n",
      "0   1.02e+04    453.28 |  409.85    103.88       0.01      0.04\n",
      "0   2.36e+04    453.28 |  382.60    110.55       0.04      0.29\n",
      "0   3.28e+04    471.39 |\n",
      "0   3.58e+04    491.33 |\n",
      "0   3.69e+04    491.33 |  491.33     76.77       0.12      0.34\n",
      "0   5.22e+04    491.33 |  389.91    131.47       0.08      0.38\n",
      "0   6.45e+04    491.33 |  381.27     93.70       0.08      0.34\n",
      "0   6.86e+04    497.62 |\n",
      "0   7.58e+04    497.62 |  467.76    108.42       0.06      0.25\n",
      "0   9.01e+04    497.62 |  491.25     57.79       0.05      0.24\n",
      "0   9.42e+04    502.69 |\n",
      "0   9.83e+04    502.69 |   25.88     17.32       0.04      0.19\n",
      "0   1.09e+05    502.69 |  457.20     82.28       0.03      0.15\n",
      "0   1.20e+05    502.69 |  465.17    131.43       0.03      0.11\n",
      "0   1.22e+05    517.77 |\n",
      "0   1.23e+05    603.26 |\n",
      "0   1.30e+05    603.26 |  518.13    115.56       0.02      0.10\n",
      "0   1.40e+05    603.26 |  585.24     74.72       0.02      0.08\n",
      "0   1.41e+05    650.71 |\n",
      "0   1.44e+05    678.62 |\n",
      "0   1.50e+05    678.62 |  670.20     51.19       0.02      0.07\n",
      "0   1.61e+05    678.62 |  511.28     74.71       0.02      0.07\n",
      "0   1.65e+05    687.72 |\n",
      "0   1.70e+05    687.72 |  680.37    131.15       0.02      0.06\n",
      "0   1.72e+05    695.29 |\n",
      "0   1.74e+05    723.14 |\n",
      "0   1.79e+05    723.14 |  703.14    161.64       0.02      0.06\n",
      "0   1.82e+05    741.93 |\n",
      "0   1.89e+05    741.93 |  704.10     87.68       0.02      0.06\n",
      "0   1.94e+05    843.15 |\n",
      "0   2.00e+05    843.15 |  806.99    132.96       0.02      0.05\n",
      "0   2.08e+05    860.73 |\n",
      "0   2.09e+05    860.73 |  860.73     79.55       0.02      0.06\n",
      "0   2.19e+05    860.73 |  593.12     70.20       0.02      0.05\n",
      "0   2.24e+05    873.92 |\n",
      "0   2.27e+05    894.62 |\n",
      "0   2.28e+05    894.62 |  894.62     85.00       0.02      0.05\n",
      "0   2.28e+05    929.87 |\n",
      "0   2.38e+05    929.87 |  893.73     99.03       0.02      0.05\n",
      "0   2.49e+05    929.87 |  898.07     52.05       0.02      0.05\n",
      "0   2.51e+05    937.25 |\n",
      "0   2.57e+05    973.38 |\n",
      "0   2.58e+05    973.38 |  973.38     84.72       0.02      0.06\n",
      "0   2.58e+05    985.41 |\n",
      "0   2.67e+05    985.41 |  918.92    175.67       0.02      0.06\n",
      "0   2.72e+05    990.17 |\n",
      "0   2.75e+05    990.17 |  939.21    226.77       0.02      0.05\n",
      "0   2.75e+05   1015.73 |\n",
      "0   2.81e+05   1044.89 |\n",
      "0   2.85e+05   1044.89 |  802.00    129.74       0.02      0.05\n",
      "0   2.95e+05   1044.89 |  982.10    202.29       0.02      0.05\n",
      "0   3.00e+05   1141.89 |\n",
      "0   3.05e+05   1141.89 | 1107.15    191.02       0.02      0.06\n",
      "0   3.08e+05   1233.90 |\n",
      "0   3.10e+05   1293.72 |\n",
      "0   3.14e+05   1293.72 | 1278.71    124.71       0.02      0.06\n",
      "0   3.14e+05   1378.98 |\n",
      "0   3.25e+05   1378.98 | 1140.19    127.98       0.02      0.06\n",
      "0   3.31e+05   1507.24 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "0   3.32e+05   1500.00 | 1507.24     44.28       8753  ########\n",
      "0   3.32e+05   1507.24 | 1507.24     44.28       0.02      0.06\n",
      "| print_norm: state_avg, state_fix_std\n",
      "| avg = np.array([-0.29033828, -0.14677154,  0.13020724, -0.37298718, -0.03495548,\n",
      "       -0.05708819, -0.04327592,  0.29114944,  0.03476769, -0.00535942,\n",
      "       -0.7587266 , -0.00348791,  0.09577169, -0.00131652,  0.22420514,\n",
      "       -0.00890298, -0.34019455,  0.00652552, -0.06252879, -0.01802376,\n",
      "       -0.03597289,  0.00914559, -0.7307812 , -0.00473083,  0.52799946,\n",
      "       -0.09903175,  0.09418215,  0.45406115], dtype=np.float32)\n",
      "| std = np.array([0.09273369, 0.3559238 , 0.17475997, 0.12848747, 0.11839754,\n",
      "       0.13503365, 0.25809973, 0.19046716, 0.57304186, 0.39437813,\n",
      "       0.6884328 , 0.30715975, 0.5511938 , 0.4521287 , 0.69636476,\n",
      "       0.32345515, 0.5518666 , 0.44550768, 0.75585157, 0.285096  ,\n",
      "       0.5646849 , 0.40661994, 0.7464572 , 0.27739546, 0.27414006,\n",
      "       0.2997069 , 0.28587168, 0.27103588], dtype=np.float32)\n",
      "| SavedDir: ./AgentModSAC/AntBulletEnv-v0_0\n",
      "| UsedTime: 8760\n"
     ]
    }
   ],
   "source": [
    "args.break_step = int(1e6 * 8)  # (5e5) 1e6, UsedTime: (15,000s) 30,000s\n",
    "args.reward_scale = 2 ** -2  # RewardRange: -50 < 0 < 2500 < 3340\n",
    "args.max_memo = 2 ** 20\n",
    "args.batch_size = 2 ** 9\n",
    "args.show_gap = 2 ** 8  # for Recorder\n",
    "args.eva_size1 = 2 ** 1  # for Recorder\n",
    "args.eva_size2 = 2 ** 3  # for Recorder\n",
    "\"TotalStep: 3e5, TargetReward: 1500, UsedTime:  8ks\"\n",
    "\"TotalStep: 6e5, TargetReward: 2500, UsedTime: 20ks\"\n",
    "\n",
    "args.rollout_num = 4\n",
    "train_and_evaluate__multiprocessing(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DEMO 4.2: choose an off-policy DRL algorithm'''\n",
    "from elegantrl.run import Arguments, train_and_evaluate__multiprocessing\n",
    "from elegantrl.env import PreprocessEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| env_name:  AntBulletEnv-v0, action space if_discrete: False\n",
      "| state_dim:   28, action_dim: 8, action_max: 1.0\n",
      "| max_step:  1000, target_reward: 2500.0\n"
     ]
    }
   ],
   "source": [
    "args = Arguments(if_on_policy=False)\n",
    "\n",
    "from elegantrl.agent import AgentPPO\n",
    "args.agent = AgentPPO()\n",
    "args.agent.if_use_gae = True\n",
    "\n",
    "\n",
    "import gym  # don't worry about 'WARN: Box bound precision lowered by casting to float32'\n",
    "import pybullet_envs  # Free PyBullet as an env alternatives of paid MuJoCo\n",
    "env_name = 'AntBulletEnv-v0'\n",
    "assert env_name in {\"AntBulletEnv-v0\",\n",
    "                    \"Walker2DBulletEnv-v0\", \n",
    "                    \"HalfCheetahBulletEnv-v0\",\n",
    "                    \"HumanoidBulletEnv-v0\", \n",
    "                    \"HumanoidFlagrunBulletEnv-v0\", \n",
    "                    \"HumanoidFlagrunHarderBulletEnv-v0\",}\n",
    "args.env = PreprocessEnv(env=gym.make(env_name))\n",
    "args.env.max_step = 2 ** 10\n",
    "args.env.target_reward = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| multiprocessing, act_workers: 4\n",
      "| multiprocessing, None:\n",
      "| GPU id: 0, cwd: ./AgentPPO/AntBulletEnv-v0_0\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       objA      objC\n",
      "0   0.00e+00      9.95 |\n",
      "0   2.90e+03     10.07 |\n",
      "0   1.31e+04     11.03 |\n",
      "0   1.71e+04     38.62 |\n",
      "0   3.12e+04    449.95 |\n",
      "0   4.28e+04    556.44 |\n",
      "0   4.66e+04    680.85 |\n",
      "0   5.66e+04    767.35 |\n",
      "0   1.21e+05    767.35 |  727.64     34.92      -0.50      1.31\n",
      "0   1.43e+05    822.35 |\n",
      "0   1.59e+05    854.50 |\n",
      "0   1.67e+05    925.34 |\n",
      "0   2.15e+05    925.34 |  738.72     70.94      -0.51      1.35\n",
      "0   3.59e+05    925.34 |  729.24     42.17      -0.52      1.16\n",
      "0   5.03e+05    925.34 |  804.23     50.80      -0.53      1.44\n",
      "0   6.47e+05    925.34 |  810.85      2.67      -0.54      1.53\n",
      "0   7.59e+05    955.70 |\n",
      "0   7.63e+05    955.70 |  955.70    139.78      -0.55      1.45\n",
      "0   7.63e+05   1007.63 |\n",
      "0   7.75e+05   1076.46 |\n",
      "0   7.83e+05   1105.29 |\n",
      "0   8.23e+05   1120.53 |\n",
      "0   8.31e+05   1120.53 | 1077.01    134.71      -0.55      2.55\n",
      "0   8.31e+05   1123.25 |\n",
      "0   8.35e+05   1246.69 |\n",
      "0   8.39e+05   1268.92 |\n",
      "0   8.67e+05   1289.60 |\n",
      "0   8.71e+05   1290.24 |\n",
      "0   8.83e+05   1297.13 |\n",
      "0   9.03e+05   1297.13 | 1237.22    170.43      -0.56      2.95\n",
      "0   9.31e+05   1304.80 |\n",
      "0   9.55e+05   1315.23 |\n",
      "0   9.59e+05   1321.96 |\n",
      "0   9.75e+05   1375.68 |\n",
      "0   9.91e+05   1375.68 | 1245.10    194.29      -0.57      3.07\n",
      "0   9.98e+05   1392.48 |\n",
      "0   1.01e+06   1398.26 |\n",
      "0   1.03e+06   1438.04 |\n",
      "0   1.03e+06   1478.15 |\n",
      "0   1.06e+06   1478.15 | 1394.60    205.62      -0.57      3.68\n",
      "0   1.08e+06   1500.80 |\n",
      "ID      Step   TargetR |    avgR      stdR   UsedTime  ########\n",
      "0   1.09e+06   1500.00 | 1500.80     60.00       2723  ########\n",
      "| print_norm: state_avg, state_fix_std\n",
      "| avg = np.array([ 2.2437108e-01,  1.7594379e+00, -5.9523153e-01,  7.2304356e-01,\n",
      "        3.8325315e-04,  3.1591564e-02,  1.5018179e-01, -3.0189317e-01,\n",
      "       -8.8278466e-01,  1.4083633e-02,  2.6370284e-01, -6.4657060e-03,\n",
      "        4.3127827e-02,  7.1345624e-03,  1.9187692e-01,  2.2706859e-02,\n",
      "       -3.4343818e-01,  6.2783328e-03,  9.3979526e-01, -2.9286307e-03,\n",
      "       -1.1007789e+00,  6.9282018e-03, -3.6013117e-01,  1.1443397e-02,\n",
      "       -3.9560556e-02,  1.9706777e-01,  5.0924861e-01,  1.9811079e-01],\n",
      "      dtype=np.float32)\n",
      "| std = np.array([0.03440588, 0.26718608, 0.19169162, 0.12820865, 0.08123839,\n",
      "       0.09460475, 0.09068087, 0.08743729, 0.44934088, 0.3307505 ,\n",
      "       0.58809304, 0.32026005, 0.48489335, 0.3660845 , 0.5758783 ,\n",
      "       0.26406568, 0.4592916 , 0.27655074, 0.2897839 , 0.13206911,\n",
      "       0.23416409, 0.23472011, 0.6330343 , 0.15222894, 0.32192105,\n",
      "       0.2713893 , 0.22952522, 0.2986782 ], dtype=np.float32)\n",
      "| SavedDir: ./AgentPPO/AntBulletEnv-v0_0\n",
      "| UsedTime: 2730\n"
     ]
    }
   ],
   "source": [
    "args.break_step = int(2e6 * 8)  # (5e5) 1e6, UsedTime: (15,000s) 30,000s\n",
    "args.reward_scale = 2 ** -2  # (-50) 0 ~ 2500 (3340)\n",
    "args.max_memo = 2 ** 11\n",
    "args.repeat_times = 2 ** 3\n",
    "args.batch_size = 2 ** 10\n",
    "args.net_dim = 2 ** 9\n",
    "args.show_gap = 2 ** 8  # for Recorder\n",
    "args.eva_size1 = 2 ** 1  # for Recorder\n",
    "args.eva_size2 = 2 ** 3  # for Recorder\n",
    "\"TotalStep:  2e6, TargetReward: 1500, UsedTime:  3ks\"\n",
    "\"TotalStep: 13e6, TargetReward: 2400, UsedTime: 21ks\"\n",
    "\n",
    "args.rollout_num = 4\n",
    "train_and_evaluate__multiprocessing(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API of ElegantRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### level 0 API: training pipeline (User-oriented)\n",
    "\n",
    "- `class Arguments` \n",
    "- `class AgentXXX`\n",
    "- `class PreprocessEnv`\n",
    "- `def train_and_evaluate(args)`\n",
    "- `def train_and_evaluate__multiprocessing(args)`\n",
    "\n",
    "---\n",
    "`class Arguments` \n",
    "Save the hyper-parameters of DRL algorithms and intiialize the training setting.\n",
    "The user should set the DRL algortithm and training enviroment for training. The other hyperparameters will set as default.\n",
    "\n",
    "---\n",
    "`class AgentXXX`\n",
    "The built-in DRL algorithms of ElegantRL. They are named as AgentXXX\n",
    "- Discrete action space: DQN, DuelingDQN, DoubleDQN, ...\n",
    "- Continuous action space (off-policy): DDPG, TD3, SAC, ...\n",
    "- Continuous action space (on-policy): PPO, GaePPO ...\n",
    "\n",
    "---\n",
    "`class PreprocessEnv`\n",
    "Preprocess the OpenAI gym standard environment for DRL.\n",
    "- DRL algorithm needs to know the env information for creating network automatically. We find and assign these variable `env.state_dim, env.action_dim, ...`\n",
    "- Some OpenAI gym standard environments are not standard enough, we adjust some continuous action range into (-1, +1). We also convert the data type of state from `float64` to `float32`.\n",
    "- Some OpenAI gym standard environments have bad state design. We do normalization on state. See more detail in `def get_avg_std__for_state_norm` in `env.py`\n",
    "\n",
    "---\n",
    "`def train_and_evaluate(args)`\n",
    "choose single processing for DRL training.\n",
    "\n",
    "\n",
    "`def train_and_evaluate__multiprocessing(args)`\n",
    "choose multiple processing for DRL training.\n",
    "\n",
    "---\n",
    "Such as: (see more in `def demo***`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "from elegantrl.run import Arguments\n",
    "args = Arguments(agent=None, env=None, gpu_id=None)\n",
    "\n",
    "# set DRL algorithms\n",
    "from elegantrl.agent import AgentPPO\n",
    "args.agent = AgentPPO()\n",
    "args.agent.if_gae = True  # change some hyperparameters of DRL algorithms\n",
    "\n",
    "# set training environment\n",
    "from elegantrl.env import PreprocessEnv\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.target_reward = -200  # set target_reward manually for env 'Pendulum-v0'\n",
    "args.env = PreprocessEnv(env=env)\n",
    "\n",
    "# start training\n",
    "train_and_evaluate(args)  # train_and_evaluate__multiprocessing(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### level 0 API: training pipeline \n",
    "\n",
    "---\n",
    "`class AgentBase` is the base class of all the DRL algorithms (both DQN variants and Actor-critic Methods).\n",
    "- `class AgentDQN(AgentBase)`\n",
    "- `class AgentDDPG(AgentBase)'\n",
    "- `class AgentTD3(AgentBase)`\n",
    "- `class AgentSAC(AgentBase)`\n",
    "- `class AgentPPO(AgentBase)`\n",
    "\n",
    "This base class should had the following attribution:\n",
    "- `init(net_dim, state_dim, action_dim)` initialize the neural network, optimizer, certierion and others for training. We explict call `init(...)` for multiprocessing instead of directly call `__init__()`.\n",
    "    - `int net_dim`: the dimension of networks (the width of neural networks)\n",
    "    - `int state_dim`: the dimension of state (the number of state vector)\n",
    "    - `int action_dim`: the dimension of action (the number of discrete action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgentXXX\n",
    "agent = AgentPPO()  # take AgentPPO as an example\n",
    "agent = agent.init(net_dim, state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### level 1 API: RAM management\n",
    "\n",
    "---\n",
    "`class ReplayBuffer` Experience Replay Buffer. We save the env transition `(state, action, reward, ... )` as a trajectory on a contiguous memory for high performance training.\n",
    "`class ReplayBufferMP` for multiprocessing.\n",
    "- `__init__(self, max_len, state_dim, action_dim, if_on_policy, if_gpu)`\n",
    "    - `max_len` the maximum capacity. First In First Out. We don't use random out because w e save the environment transition as an ordered trajectory.\n",
    "    - `if_on_policy` switch to on-policy-DRl-mode or off-policy.\n",
    "    - `if_gpu` switch to `torch.tensor`-GPU-mode or `numpy.ndarray`-CPU.\n",
    "- `append_buffer(state, other)` same as list.append() in Python. `state, other` are `nd.array` or `torch.tensor`. \n",
    "- `extend_buffer(state, other)` same as list.extend() in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReplayBuffer\n",
    "\n",
    "state = np.array(state_dim)\n",
    "action = np.array(action_dim)\n",
    "other = \n",
    "\n",
    "buffer = ReplayBuffer(max_len, state_dim, action_dim, if_on_policy, if_gpu)\n",
    "buffer.append(state, other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name list of ElegantRL\n",
    "\n",
    "enviroment\n",
    "- `str env_name`: the environment name. Such as `'CartPole-v0', 'LunarLander-v2'`\n",
    "- `int state_dim`: the dimension of state (the number of state vector)\n",
    "- `int action_dim`: the dimension of action (the number of discrete action)\n",
    "- `int max_step`: the max step of an episode. The actor will break this episode of environment exploration when `done=Ture` or `steps > max_step`. \n",
    "- `bool if_discrete`: if swith to di\n",
    "\n",
    "training \n",
    "- `int net_dim`: the dimension of networks (the width of neural networks)\n",
    "\n",
    "other:\n",
    "- `bool if_xxx`: a Boolean value. Call it a `flag` in English?\n",
    "- `bool if_on_policy` it shows that it is an on-policy algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API List\n",
    "\n",
    "---\n",
    "## `class Arguement()` \n",
    "set hyperparameters\n",
    "\n",
    "### `init_before_training(self, if_main)` \n",
    "prepare training environment.\n",
    "\n",
    "- `bool if_main`: build current work directory\n",
    "\n",
    "---\n",
    "### `train_and_evaluate(args)` \n",
    "single processing, `args=Arguement()`\n",
    "\n",
    "### `train_and_evaluate__multiprocessing(args)` \n",
    "multiprocessing, `args=Arguement()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## `class AgentBase()` \n",
    "\n",
    "### `__init__()` \n",
    "default initialize\n",
    "\n",
    "\n",
    "### `init(self, net_dim, state_dim, action_dim)` \n",
    "explict call init() to build networks\n",
    "\n",
    "- `int net_dim`: the dimension of networks \n",
    "\n",
    "- `int state_dim`: the dimension of state \n",
    "\n",
    "- `int action_dim`: the dimension of action (or the number of discrete action)\n",
    "\n",
    "\n",
    "### `select_action(self, state)` \n",
    "select action for exploration\n",
    "\n",
    "- `array state`: the shape of state is `(state_dim, )`\n",
    "\n",
    "\n",
    "### `store_transition(self, env, buffer, target_step, reward_scale, gamma)`\n",
    "store transition (state, action, reward, ...) to ReplayBuffer\n",
    "\n",
    "- `env`: DRL training environment, it has `.reset()` and `.step()`\n",
    "\n",
    "- `buffer`: experience replay buffer. ReplayBuffer has `.append_buffer()` \n",
    "\n",
    "- `int target_step`: number of target steps plan to collect in env\n",
    "\n",
    "- `float reward_scale`: scale the reward size\n",
    "\n",
    "- `float gamma`: discount factor\n",
    "\n",
    "plan to move `reward_scale` and `gamma` to `__init__()`\n",
    "\n",
    "\n",
    "### `update_net(self, buffer, target_step, batch_size, repeat_times)`\n",
    "update networks using sampling data from ReplayBuffer\n",
    "\n",
    "- `buffer`: experience replay buffer. ReplayBuffer has `.sample_batch()`\n",
    "\n",
    "- `int target_step`: number of target steps that add to ReplayBuffer\n",
    "\n",
    "- `int batch_size`: number of samples for stochastic gradient decent\n",
    "\n",
    "\n",
    "### `save_load_model(self, cwd, if_save)`\n",
    "save neural network to cwd\n",
    "\n",
    "- `str cwd`: current working directory\n",
    "\n",
    "- `bool if_save` save or load model\n",
    "\n",
    "\n",
    "### `soft_update(self, target_net, current_net)` \n",
    "soft target update using self.tau (set as `2**-8` in default)\n",
    "\n",
    "- `target_net`: update via soft target update of `current_net`\n",
    "\n",
    "- `current_net`: update via gradient decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## `class ReplayBuffer`\n",
    "experience replay buffer\n",
    "\n",
    "### `_init__(self, max_len, state_dim, action_dim, if_on_policy, if_gpu)`\n",
    "creat a continuous memory space to store data\n",
    "\n",
    "- `max_len`: maximum capacity, First In First Out.\n",
    "\n",
    "- `int state_dim`: the dimension of state \n",
    "\n",
    "- `int action_dim`: the dimension of action (`action_dim=1` for discrete action)\n",
    "\n",
    "- `bool if_on_policy`: on-policy or off-policy\n",
    "\n",
    "- `if_gpu`: creat memory space on CPU RAM or GPU RAM\n",
    "\n",
    "### `append_buffer(self, state, other)`\n",
    "append to ReplayBuffer, same as `list.append()`\n",
    "\n",
    "- `array state`: the shape is `(state_dim, )`\n",
    "\n",
    "- `array other`: the shape is `(other_dim, )`, including action, reward, ...\n",
    "\n",
    "\n",
    "### `extend_buffer(self, state, other)`\n",
    "extend to ReplayBuffer, same as `list.extend()`\n",
    "\n",
    "- `array state`: the shape is `(-1, state_dim)`\n",
    "\n",
    "- `array other`: the shape is `(-1, other_dim)`, including action, reward, ...\n",
    "\n",
    "\n",
    "### `sample_batch(self, batch_size)`\n",
    "sample a batch of data from ReplayBuffer randomly for stochastic gradient decent\n",
    "\n",
    "- `int batch_size`: number of data in a batch\n",
    "\n",
    "\n",
    "### `sample_for_ppo(self)`\n",
    "sample all the data from ReplayBuffer.\n",
    "\n",
    "- return `float reward`\n",
    "\n",
    "- return `float mask`\n",
    "\n",
    "- return `array action`\n",
    "\n",
    "- return `array noise`\n",
    "\n",
    "- return `array state`\n",
    "\n",
    "### `update__now_len__before_sample(self)`\n",
    "update `now_len` (pointer) before sample data form ReplayBuffer\n",
    "\n",
    "### `empty_memories__before_explore(self)`\n",
    "empty the memories of ReplayBuffer before exploring for on-policy\n",
    "\n",
    "### `print_state_norm(self)`\n",
    "print the `avg` and `std` for state normalization. compute using the state in ReplayBuffer after finishing the training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `class Preprocess(gym.Wrapper)`\n",
    "\n",
    "### `__init__(self, env)` \n",
    "get environemnt information\n",
    "\n",
    "- `reset(self)` return `state`\n",
    "\n",
    "- `step(self, action)` return `(next_state, reward, done, dict)`\n",
    "\n",
    "\n",
    "- `str env_name`: for example LunarLander-v2\n",
    "\n",
    "- `int net_dim`: the dimension of networks \n",
    "\n",
    "- `int state_dim`: the dimension of state \n",
    "\n",
    "- `int action_dim`: the dimension of action (or the number of discrete action)\n",
    "\n",
    "- `int action_max`: the max range of continuous action. action_max=1 when it is discrete action\n",
    "\n",
    "- `int max_step`: the max step of an episode \n",
    "\n",
    "- `bool if_discrete`: discrete or continuous action space\n",
    "\n",
    "- `float target_reward`: the gold score of this environment\n",
    "\n",
    "- `array neg_state_avg`: for state normalization\n",
    "\n",
    "- `array div_state_std`: for state normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
